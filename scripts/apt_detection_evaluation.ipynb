{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb1169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.exceptions import InconsistentVersionWarning\n",
    "\n",
    "# Your imports\n",
    "from llm2 import LLMAnalyzer\n",
    "from process4 import GraphProcessor\n",
    "from graph3 import GraphAnalyzer\n",
    "from dscreate import DatasetCreation\n",
    "from deviation import DeviationAnalyzerFrequency\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Configuration\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\", category=InconsistentVersionWarning)\n",
    "\n",
    "# Dataset configuration\n",
    "dataset = 'theia'\n",
    "dataset_file = f'{dataset}_test_logs.pkl'\n",
    "lower_bound = \"2018-04-10 12:00\" \n",
    "upper_bound = \"2018-04-12 23:55\"\n",
    "interval = 30\n",
    "sliding_window = 15\n",
    "\n",
    "api_key=\"\"\n",
    "endpoint=\"\"\n",
    "deployment_name=\"\"\n",
    "\n",
    "os.environ['AZURE_OPENAI_API_KEY'] = api_key\n",
    "os.environ['AZURE_OPENAI_ENDPOINT'] = endpoint\n",
    "os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'] = deployment_name\n",
    "\n",
    "# Directory setup\n",
    "ANOMALY_DIR = 'anomaly'\n",
    "PREPROCESSED_DIR = 'preprocessed'\n",
    "\n",
    "def create_directories():\n",
    "    \"\"\"Create necessary directories if they don't exist\"\"\"\n",
    "    for directory in [ANOMALY_DIR, PREPROCESSED_DIR]:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"✓ Created directory: {directory}\")\n",
    "\n",
    "def perform_preprocessing(dataset_name, csv_file, deviation_file, output_file, graph_analyzer):\n",
    "    \"\"\"\n",
    "    Perform anomaly detection and graph analysis on a single file\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset\n",
    "        csv_file: Path to input CSV file\n",
    "        deviation_file: Path to save anomalous logs\n",
    "        output_file: Path to save processed results\n",
    "        graph_analyzer: GraphAnalyzer instance\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistics about processing\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'file': os.path.basename(csv_file),\n",
    "        'status': 'failed',\n",
    "        'anomalies_found': 0,\n",
    "        'processed_rows': 0,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if input file exists and is not empty\n",
    "        if not os.path.exists(csv_file):\n",
    "            stats['error'] = f\"File not found: {csv_file}\"\n",
    "            stats['status'] = 'error'\n",
    "            return stats\n",
    "            \n",
    "        # Check file size\n",
    "        file_size = os.path.getsize(csv_file)\n",
    "        if file_size == 0:\n",
    "            print(f\"  ⚠️  File is empty, skipping\")\n",
    "            stats['status'] = 'empty_file'\n",
    "            return stats\n",
    "        \n",
    "        # Step 1: Anomaly Detection\n",
    "        print(f\"  [1/3] Running anomaly detection...\")\n",
    "        analyzer = DeviationAnalyzerFrequency(dataset_name, csv_file)\n",
    "        graph_df = analyzer.analyze()\n",
    "        \n",
    "        if graph_df is None or graph_df.empty:\n",
    "            print(f\"  ⚠️  No anomalies detected\")\n",
    "            stats['status'] = 'no_anomalies'\n",
    "            return stats\n",
    "        \n",
    "        stats['anomalies_found'] = len(graph_df)\n",
    "        print(f\"  ✓ Found {stats['anomalies_found']} anomalous entries\")\n",
    "        \n",
    "        # Save anomalies\n",
    "        graph_df.to_csv(deviation_file, index=False)\n",
    "        print(f\"  ✓ Saved anomalies to: {os.path.basename(deviation_file)}\")\n",
    "        \n",
    "        # Step 2: Graph Analysis\n",
    "        print(f\"  [2/3] Running graph analysis...\")\n",
    "        df_filtered = graph_analyzer.analyze(graph_df)\n",
    "        \n",
    "        if df_filtered is None or df_filtered.empty:\n",
    "            print(f\"  ⚠️  No data after graph analysis\")\n",
    "            stats['status'] = 'filtered_out'\n",
    "            return stats\n",
    "        \n",
    "        stats['processed_rows'] = len(df_filtered)\n",
    "        print(f\"  ✓ Processed {stats['processed_rows']} rows\")\n",
    "        \n",
    "        # Step 3: Save Results\n",
    "        print(f\"  [3/3] Saving processed results...\")\n",
    "        df_filtered.to_csv(output_file, index=False)\n",
    "        print(f\"  ✓ Saved to: {os.path.basename(output_file)}\")\n",
    "        \n",
    "        stats['status'] = 'success'\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error: {str(e)}\")\n",
    "        stats['error'] = str(e)\n",
    "        stats['status'] = 'error'\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def traverse_directory(dataset_name):\n",
    "    \"\"\"\n",
    "    Process all CSV files in the dataset directory\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset directory\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (processing_stats, graph_analysis_results)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING ANOMALY DETECTION & PREPROCESSING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize graph analyzer once\n",
    "    graph_analyzer = GraphAnalyzer()\n",
    "    \n",
    "    # Get all CSV files\n",
    "    if not os.path.exists(dataset_name):\n",
    "        print(f\"⚠️  Dataset directory '{dataset_name}' not found\")\n",
    "        return [], (None, None)\n",
    "        \n",
    "    csv_files = [f for f in os.listdir(dataset_name) if f.endswith('.csv')]\n",
    "    total_files = len(csv_files)\n",
    "    \n",
    "    if total_files == 0:\n",
    "        print(f\"⚠️  No CSV files found in '{dataset_name}' directory\")\n",
    "        return [], (None, None)\n",
    "    \n",
    "    print(f\"\\nFound {total_files} CSV files to process\\n\")\n",
    "    \n",
    "    # Track statistics\n",
    "    all_stats = []\n",
    "    \n",
    "    # Process each file\n",
    "    for idx, file in enumerate(csv_files, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing [{idx}/{total_files}]: {file}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        file_path = os.path.join(dataset_name, file)\n",
    "        deviation_file = os.path.join(ANOMALY_DIR, f'anomalous_{file}')\n",
    "        result_file = os.path.join(PREPROCESSED_DIR, f'processed_{file}')\n",
    "        \n",
    "        stats = perform_preprocessing(\n",
    "            dataset_name, \n",
    "            file_path, \n",
    "            deviation_file, \n",
    "            result_file, \n",
    "            graph_analyzer\n",
    "        )\n",
    "        all_stats.append(stats)\n",
    "    \n",
    "    # Finalize graph analysis\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINALIZING GRAPH ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        results, results2 = graph_analyzer.finalize_analysis()\n",
    "        print(\"✓ Graph analysis finalized successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error finalizing graph analysis: {str(e)}\")\n",
    "        results, results2 = None, None\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Print summary\n",
    "    print_summary(all_stats)\n",
    "    \n",
    "    return all_stats, (results, results2)\n",
    "\n",
    "def print_summary(stats_list):\n",
    "    \"\"\"Print summary of processing results\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PROCESSING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    total = len(stats_list)\n",
    "    successful = sum(1 for s in stats_list if s['status'] == 'success')\n",
    "    no_anomalies = sum(1 for s in stats_list if s['status'] == 'no_anomalies')\n",
    "    filtered = sum(1 for s in stats_list if s['status'] == 'filtered_out')\n",
    "    errors = sum(1 for s in stats_list if s['status'] == 'error')\n",
    "    empty = sum(1 for s in stats_list if s['status'] == 'empty_file')\n",
    "    \n",
    "    total_anomalies = sum(s['anomalies_found'] for s in stats_list)\n",
    "    total_processed = sum(s['processed_rows'] for s in stats_list)\n",
    "    \n",
    "    print(f\"\\nTotal files: {total}\")\n",
    "    print(f\"  ✓ Successfully processed: {successful}\")\n",
    "    print(f\"  ⚠️  No anomalies found: {no_anomalies}\")\n",
    "    print(f\"  ⚠️  Filtered out: {filtered}\")\n",
    "    print(f\"  ⚠️  Empty files: {empty}\")\n",
    "    print(f\"  ❌ Errors: {errors}\")\n",
    "    \n",
    "    print(f\"\\nAnomaly Detection:\")\n",
    "    print(f\"  Total anomalies found: {total_anomalies:,}\")\n",
    "    print(f\"  Total rows processed: {total_processed:,}\")\n",
    "    \n",
    "    # Show errors if any\n",
    "    if errors > 0:\n",
    "        print(f\"\\nFiles with errors:\")\n",
    "        for s in stats_list:\n",
    "            if s['status'] == 'error':\n",
    "                print(f\"  - {s['file']}: {s['error']}\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "def validate_preprocessed_files():\n",
    "    \"\"\"Validate preprocessed files before LLM analysis\"\"\"\n",
    "    print(\"\\nValidating preprocessed files...\")\n",
    "    \n",
    "    if not os.path.exists(PREPROCESSED_DIR):\n",
    "        return []\n",
    "    \n",
    "    valid_files = []\n",
    "    invalid_files = []\n",
    "    \n",
    "    for file in os.listdir(PREPROCESSED_DIR):\n",
    "        if not file.endswith('.csv'):\n",
    "            continue\n",
    "            \n",
    "        file_path = os.path.join(PREPROCESSED_DIR, file)\n",
    "        \n",
    "        try:\n",
    "            # Check file size\n",
    "            if os.path.getsize(file_path) == 0:\n",
    "                invalid_files.append((file, \"Empty file\"))\n",
    "                continue\n",
    "            \n",
    "            # Try to read the file\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            if df.empty:\n",
    "                invalid_files.append((file, \"No data\"))\n",
    "                continue\n",
    "            \n",
    "            # Check for required columns (adjust based on your needs)\n",
    "            required_cols = ['event', 'processName', 'objectData', 'processUUID']\n",
    "            missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "            \n",
    "            if missing_cols:\n",
    "                invalid_files.append((file, f\"Missing columns: {missing_cols}\"))\n",
    "                continue\n",
    "            \n",
    "            valid_files.append(file_path)\n",
    "            print(f\"  ✓ {file}: {len(df)} rows\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            invalid_files.append((file, str(e)))\n",
    "    \n",
    "    if invalid_files:\n",
    "        print(f\"\\n⚠️  Invalid files ({len(invalid_files)}):\")\n",
    "        for file, reason in invalid_files:\n",
    "            print(f\"  - {file}: {reason}\")\n",
    "    \n",
    "    print(f\"\\n✓ Found {len(valid_files)} valid preprocessed files\")\n",
    "    return valid_files\n",
    "\n",
    "def run_llm_analysis():\n",
    "    \"\"\"Run LLM analysis on preprocessed data\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING LLM ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Validate preprocessed files first\n",
    "        valid_files = validate_preprocessed_files()\n",
    "        \n",
    "        if not valid_files:\n",
    "            print(\"\\n⚠️  No valid preprocessed files found. Skipping LLM analysis.\")\n",
    "            return\n",
    "        \n",
    "        # Check Azure OpenAI credentials\n",
    "        api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "        \n",
    "        if not all([api_key, endpoint, deployment]):\n",
    "            print(\"\\n⚠️  Azure OpenAI credentials not set. Skipping LLM analysis.\")\n",
    "            print(\"Required environment variables:\")\n",
    "            print(f\"  - AZURE_OPENAI_API_KEY: {'✓' if api_key else '✗'}\")\n",
    "            print(f\"  - AZURE_OPENAI_ENDPOINT: {'✓' if endpoint else '✗'}\")\n",
    "            print(f\"  - AZURE_OPENAI_DEPLOYMENT_NAME: {'✓' if deployment else '✗'}\")\n",
    "            return\n",
    "        \n",
    "        # Initialize Azure OpenAI client\n",
    "        print(\"\\nInitializing Azure OpenAI client...\")\n",
    "        client = AzureOpenAI(\n",
    "            api_key=api_key,  \n",
    "            api_version=\"2024-02-01\",\n",
    "            azure_endpoint=endpoint,\n",
    "        )\n",
    "        \n",
    "        # Run LLM analysis\n",
    "        print(\"Initializing LLM analyzer...\")\n",
    "        llm_analyzer = LLMAnalyzer(client, deployment)\n",
    "        analyzer = GraphProcessor(llm_analyzer, graph_file='provenance_graph.gpickle')\n",
    "        \n",
    "        print(f\"\\nAnalyzing {len(valid_files)} files in '{PREPROCESSED_DIR}'...\")\n",
    "        analyzer.analyze_directory(PREPROCESSED_DIR)\n",
    "        \n",
    "        print(\"Finalizing analysis...\")\n",
    "        analyzer.finalize()\n",
    "        \n",
    "        print(\"\\n✓ LLM analysis completed successfully\")\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        print(f\"\\n❌ Attribute Error: {str(e)}\")\n",
    "        print(\"\\nThis error typically means:\")\n",
    "        print(\"  1. A graph object is None when it should have data\")\n",
    "        print(\"  2. The preprocessed files may not have the expected structure\")\n",
    "        print(\"  3. The graph analysis returned None\")\n",
    "        print(\"\\nDebugging info:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during LLM analysis: {str(e)}\")\n",
    "        print(\"\\nFull traceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Step 1: Create directories\n",
    "        print(\"=\"*60)\n",
    "        print(\"INITIALIZING\")\n",
    "        print(\"=\"*60)\n",
    "        create_directories()\n",
    "        \n",
    "        # Step 2: Create dataset\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CREATING DATASET\")\n",
    "        print(\"=\"*60)\n",
    "        dataset_creator = DatasetCreation(\n",
    "            dataset, \n",
    "            dataset_file, \n",
    "            lower_bound, \n",
    "            upper_bound, \n",
    "            interval, \n",
    "            sliding_window\n",
    "        )\n",
    "        dataset_creator.create_dataset()\n",
    "        print(\"✓ Dataset created successfully\")\n",
    "        \n",
    "        # Step 3: Process files (anomaly detection + graph analysis)\n",
    "        stats, graph_results = traverse_directory(dataset)\n",
    "        \n",
    "        # Step 4: Run LLM analysis if there are processed files\n",
    "        run_llm_analysis()\n",
    "        \n",
    "        # Final message\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✅ ALL PROCESSING COMPLETED\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Results saved in:\")\n",
    "        print(f\"  - Anomalies: {ANOMALY_DIR}/\")\n",
    "        print(f\"  - Preprocessed: {PREPROCESSED_DIR}/\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n⚠️  Process interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Critical error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
